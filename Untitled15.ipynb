{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT4sLJK9swszOzxNPelkqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gediyo/CartPole_balancing/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import random\n",
        "import time\n"
      ],
      "metadata": {
        "id": "FC3oasjl6402"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the cartpole environment\n",
        "env = gym.make('CartPole-v0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHlHqZ4865ge",
        "outputId": "be6156e9-75cc-46c6-ba5c-695bc030eb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# building a NN  model\n",
        "def build_model(state_size, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=state_size, name = \"Layer1\", activation='relu'))\n",
        "    model.add(Dense(24, name = \"Layer2\", activation='relu'))\n",
        "    model.add(Dense(action_size, name = \"Layer3\", activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=0.1))\n",
        "    return model\n",
        "\n",
        "p_inputSpace = 4        # observation space\n",
        "p_outputSpace = env.action_space.n            # action space\n",
        "\n",
        "model = build_model(p_inputSpace, p_outputSpace)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyj4_d2A67ou",
        "outputId": "214dbebe-1ee8-4d20-a70b-d421ae8ebdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Layer1 (Dense)              (None, 24)                120       \n",
            "                                                                 \n",
            " Layer2 (Dense)              (None, 24)                600       \n",
            "                                                                 \n",
            " Layer3 (Dense)              (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 770 (3.01 KB)\n",
            "Trainable params: 770 (3.01 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "epsilon = 0.1\n",
        "\n",
        "def select_action(state):\n",
        "  \"\"\"\n",
        "  action selection based on epsilon greedy policy\n",
        "  state: current state\n",
        "  \"\"\"\n",
        "\n",
        "  if random.random() < epsilon:\n",
        "    return env.action_space.sample()\n",
        "  else:\n",
        "    return np.argmax([model.predict(state.reshape(1,4), verbose= 0)])\n",
        "\n",
        "\n",
        "gamma = 0.99\n",
        "n = 4\n",
        "episodes = 5e3\n",
        "start_timer = time.time()\n",
        "episode = 1\n",
        "\n",
        "\n",
        "while episode < episodes:\n",
        "\n",
        "  episode_tracker = []\n",
        "\n",
        "  # reset the environment\n",
        "  state = env.reset()\n",
        "  # select an action based epsilon greedy policy\n",
        "  action = select_action(state)\n",
        "\n",
        "  episode_tracker.append([0,state,action])\n",
        "\n",
        "  T = float('inf') # terminal state\n",
        "\n",
        "  t = 0 #initialize the time-step counter t\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if t<T:\n",
        "      next_state, reward, done, info = env.step(action) # taking the action\n",
        "\n",
        "      episode_tracker.append([reward, next_state])\n",
        "\n",
        "      if done:\n",
        "        T = t+1\n",
        "      else:\n",
        "        # select the next action based on epsilon greedy policy\n",
        "        next_action = select_action(next_state)\n",
        "        episode_tracker[-1].append(next_action)\n",
        "\n",
        "    tau = t - n + 1\n",
        "\n",
        "    if tau >= 0:\n",
        "      G = sum([(gamma**(i-tau-1))*episode_tracker[i][0] for i in range(tau + 1, min(tau + n, T) + 1)]) # compute the return\n",
        "\n",
        "      if tau + n < T:\n",
        "        s = episode_tracker[tau + n][1] # state at tau+n time step\n",
        "        a = episode_tracker[tau + n][2] # action at tau+n time step\n",
        "        G += (gamma**n) * (model.predict(s.reshape(1,4), verbose = 0)[0][a])\n",
        "\n",
        "      s_tau = episode_tracker[tau][1] # state at tau time step\n",
        "      a_tau = episode_tracker[tau][2] # action at tau time step\n",
        "\n",
        "\n",
        "      ####### try 1 ###########\n",
        "\n",
        "      # with tf.GradientTape() as tape:\n",
        "      #   predictions = model(np.array([np.append(s_tau, a_tau)]),training=True)\n",
        "      #   ppError = f_ppError(G,predictions)\n",
        "      # grads = tape.gradient(ppError,model.trainable_weights) # obtaining the gradient of the mean square error\n",
        "      #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "      ####### try 2 ###########\n",
        "\n",
        "      G =  np.array([G]).reshape(1, 1)\n",
        "      prediction = model.predict(s_tau.reshape(1,4), verbose = 0)\n",
        "      prediction[0][a_tau] = G\n",
        "      model.fit(s_tau.reshape(1,4), prediction, epochs=1, verbose=0)\n",
        "\n",
        "      #######  try 3 ##########\n",
        "\n",
        "\n",
        "      # weights = model.get_weights()\n",
        "\n",
        "      # Calculate the prediction\n",
        "      # prediction = model.predict(np.array([np.append(s_tau,a_tau)]).reshape(1,5), verbose = 0)\n",
        "\n",
        "      # # Calculate the error (assuming 'G' is a scalar)\n",
        "      # error = G - prediction\n",
        "\n",
        "      # Assuming 'grads' is a list or NumPy array of gradients, update each weight\n",
        "      # for i in range(len(weights)):\n",
        "      #     weights[i] += alpha * error * grads[i].numpy().squeeze()\n",
        "\n",
        "      # model.set_weights(weights)\n",
        "\n",
        "    t += 1\n",
        "\n",
        "\n",
        "    if tau == T-1:  # when the update time reaches terminal state, break\n",
        "      break\n",
        "  episode += 1\n",
        "  if episode % 100 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Saving the weights\n",
        "model.save_weights('/content/drive/My Drive/model_weights_v2.h5')\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th0LAei16_gX",
        "outputId": "db36e0cb-075b-4e64-9d18-8c08a32b8421"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-4-29c29886adf2>:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  prediction[0][a_tau] = G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:  100  time:  00:03:19\n",
            "Episode:  200  time:  00:06:28\n",
            "Episode:  300  time:  00:09:43\n",
            "Episode:  400  time:  00:12:54\n",
            "Episode:  500  time:  00:16:13\n",
            "Episode:  600  time:  00:19:34\n",
            "Episode:  700  time:  00:22:58\n",
            "Episode:  800  time:  00:26:22\n",
            "Episode:  900  time:  00:29:47\n",
            "Episode:  1000  time:  00:33:11\n",
            "Episode:  1100  time:  00:36:38\n",
            "Episode:  1200  time:  00:40:04\n",
            "Episode:  1300  time:  00:43:30\n",
            "Episode:  1400  time:  00:46:55\n",
            "Episode:  1500  time:  00:50:15\n",
            "Episode:  1600  time:  00:53:38\n",
            "Episode:  1700  time:  00:57:05\n",
            "Episode:  1800  time:  01:00:23\n",
            "Episode:  1900  time:  01:03:48\n",
            "Episode:  2000  time:  01:07:13\n",
            "Episode:  2100  time:  01:10:34\n",
            "Episode:  2200  time:  01:13:58\n",
            "Episode:  2300  time:  01:17:17\n",
            "Episode:  2400  time:  01:20:40\n",
            "Episode:  2500  time:  01:23:58\n",
            "Episode:  2600  time:  01:27:17\n",
            "Episode:  2700  time:  01:30:42\n",
            "Episode:  2800  time:  01:34:02\n",
            "Episode:  2900  time:  01:37:21\n",
            "Episode:  3000  time:  01:40:47\n",
            "Episode:  3100  time:  01:44:08\n",
            "Episode:  3200  time:  01:47:33\n",
            "Episode:  3300  time:  01:50:53\n",
            "Episode:  3400  time:  01:54:17\n",
            "Episode:  3500  time:  01:57:38\n",
            "Episode:  3600  time:  02:01:00\n",
            "Episode:  3700  time:  02:04:26\n",
            "Episode:  3800  time:  02:07:50\n",
            "Episode:  3900  time:  02:11:11\n",
            "Episode:  4000  time:  02:14:40\n",
            "Episode:  4100  time:  02:18:00\n",
            "Episode:  4200  time:  02:21:21\n",
            "Episode:  4300  time:  02:24:42\n",
            "Episode:  4400  time:  02:28:05\n",
            "Episode:  4500  time:  02:31:22\n",
            "Episode:  4600  time:  02:34:44\n",
            "Episode:  4700  time:  02:38:03\n",
            "Episode:  4800  time:  02:41:24\n",
            "Episode:  4900  time:  02:44:49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  #test\n",
        "  steps_to_solution = []\n",
        "\n",
        "  for j in range(100):\n",
        "    state = env.reset()\n",
        "\n",
        "    for i in range(1,50000):\n",
        "\n",
        "      action = np.argmax(model.predict(state.reshape(1,4), verbose = 0))\n",
        "      # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "      state, reward, done, info = env.step(action)\n",
        "\n",
        "      if done:\n",
        "        #print (\"done\")\n",
        "        steps_to_solution.append(i)\n",
        "        break\n",
        "  print(steps_to_solution)\n",
        "  avg_step = np.mean(steps_to_solution)\n",
        "  print(\"Average steps to solution per 100 episodes: \",avg_step)\n",
        "\n",
        "  # ipythondisplay.clear_output(wait=True)\n",
        "  env.close()"
      ],
      "metadata": {
        "id": "wEgabPzZBCit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}