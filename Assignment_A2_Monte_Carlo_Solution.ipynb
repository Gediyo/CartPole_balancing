{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part B"
      ],
      "metadata": {
        "id": "N8EXb_zQHHPC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aZEWzDnmXnBa"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "djojM8nMYTrY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "gvZnmgMWYV45"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "ftWl23gCYtya",
        "outputId": "9b66f03c-9f0f-4181-8824-df7de267247b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n",
            "Fetched 7,813 kB in 3s (2,795 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 121913 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NuH6DskPYYCP",
        "outputId": "06ea19d7-168c-406a-d8a9-7bdb9674dca2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7dd65f311cf0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XOW8HPVXYZxU",
        "outputId": "069df1a9-0c6d-4369-9f6b-269c5b19c2e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations that were run: 22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part 1: initialization\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# discritizing function for the observation space\n",
        "\n",
        "def discretize(value,min_value,max_value,num_bins):\n",
        "  \"\"\"\n",
        "  Discretizes a value using a uniform grid, clamping values outside of the grid range.\n",
        "\n",
        "  Args:\n",
        "    value: The value to discretize.\n",
        "    min_value: The minimum value of the grid.\n",
        "    max_value: The maximum value of the grid.\n",
        "    num_bins: The number of bins to use.\n",
        "\n",
        "  Returns:\n",
        "    The discretized value.\n",
        "  \"\"\"\n",
        "  value = min(max(value, min_value), max_value)\n",
        "\n",
        "\n",
        "  bin_size = (max_value - min_value) / num_bins\n",
        "  bin_index = int((value - min_value) / bin_size)\n",
        "\n",
        "  return bin_index\n",
        "\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  cart_position, cart_velocity, pole_angle, pole_velocity = observation\n",
        "\n",
        "  state = [discretize(cart_position, -4.8, 4.8, 5),\n",
        "           discretize(cart_velocity, -2, 2, 7),\n",
        "           discretize(pole_angle, -0.418, 0.418, 9),\n",
        "           discretize(pole_velocity, -5, 5, 7)\n",
        "           ]\n",
        "  return tuple(state)\n",
        "\n",
        "\n",
        "# initializing the action-value function\n",
        "\n",
        "\n",
        "# defining the number of bins and therefore the vlaues of the the observation space\n",
        "cart_position_bins = 6\n",
        "cart_velocity_bins = 8\n",
        "pole_angle_bins = 10\n",
        "pole_velocity_bins = 8\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product([x for x in range(cart_position_bins)],\n",
        "                                [x for x in range(cart_velocity_bins)],\n",
        "                                [x for x in range(pole_angle_bins)],\n",
        "                                [x for x in range(pole_velocity_bins)])\n",
        "\n",
        "# initialize the action-value function\n",
        "q = {}\n",
        "c = {}\n",
        "policy = {}\n",
        "b = {} # behaviour policy\n",
        "\n",
        "for state in states:\n",
        "  c[state] = np.zeros((env.action_space.n)) # initialize the sum for the s-a pair\n",
        "  b[state] = np.full(((env.action_space.n)),1/env.action_space.n) # initialize the behaviour policy\n",
        "  q[state] = np.random.random((env.action_space.n)) # initialize the action-value function\n",
        "  policy[state] = np.zeros((env.action_space.n)) # setting zero probabilities for all actions\n",
        "\n",
        "  best_action = np.argmax(q[state])  # selecting the best action based on the action-value function\n",
        "  policy[state][best_action] = 1  # setting 1 to the probability of the best action\n",
        "\n"
      ],
      "metadata": {
        "id": "WKsP0moS9VZu",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 2 - Iteration\n",
        "\n",
        "start_timer = time.time() # starting the timer\n",
        "\n",
        "\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.1\n",
        "decay_rate = 0.99\n",
        "\n",
        "\n",
        "episodes = 1e5\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "\n",
        "  # epsilon-greedy policy with an 𝜖 annealing strategy\n",
        "  epsilon = max(epsilon_end, epsilon_start * (decay_rate ** (episode//1e3)))\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  # initializing a dictionary to save the episode's trajectory\n",
        "  episode_trajectory = [[0, state]] # starting reward (0) and state\n",
        "\n",
        "  # generating an episodes using a an epsilon greedy policy\n",
        "  for t in range(1,50000):\n",
        "\n",
        "    # selecting action for the episode based on e-soft policy b\n",
        "    rnd_num = np.random.uniform(0,1) # generatind a random number\n",
        "    cumlative_action_prob = np.cumsum(b[state])  # cumlative action probabilities\n",
        "    action = np.argmax(cumlative_action_prob > rnd_num) # selecting the action based on the random number\n",
        "\n",
        "    episode_trajectory[-1].append(action) # appending the action to the episode trajectory\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs) # discretize the observation\n",
        "    episode_trajectory.append([reward,state]) # append the reward and the state to the episode trajectory\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  # initializing the return and the weight\n",
        "  G = 0\n",
        "  W = 1\n",
        "  gamma = 0.9 # discount factor\n",
        "\n",
        "  # updating the action-value function\n",
        "  for t in range(len(episode_trajectory)-2,-1,-1):\n",
        "    G = gamma*G  + episode_trajectory[t+1][0]\n",
        "    state = episode_trajectory[t][1]\n",
        "    action = episode_trajectory[t][2]\n",
        "    c[state][action] += W  # updating the sum for the s-a pair\n",
        "    q[state][action] += (W/c[state][action]) * (G - q[state][action]) # incremental averaging\n",
        "\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "\n",
        "    policy[state] = np.zeros(env.action_space.n) # resetting the policy for the given state\n",
        "    policy[state][best_action] = 1 # setting the probability of the best action to 1\n",
        "\n",
        "    b[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "    b[state][best_action] += (1-epsilon)\n",
        "\n",
        "    if action != best_action:\n",
        "      break\n",
        "    W = W/b[state][action]\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i1XGp4dFC39Z",
        "outputId": "080aec59-9fe9-4274-be53-396b3cf788aa"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:  10000  time:  00:00:24\n",
            "Episode:  20000  time:  00:00:53\n",
            "Episode:  30000  time:  00:01:28\n",
            "Episode:  40000  time:  00:02:06\n",
            "Episode:  50000  time:  00:02:47\n",
            "Episode:  60000  time:  00:03:32\n",
            "Episode:  70000  time:  00:04:19\n",
            "Episode:  80000  time:  00:05:10\n",
            "Episode:  90000  time:  00:06:09\n",
            "Episode:  100000  time:  00:07:18\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "obs = env.reset()\n",
        "state = discretize_observation(obs)\n",
        "# prev_screen = env.render(mode='rgb_array')\n",
        "# plt.imshow(prev_screen)\n",
        "reward = 0\n",
        "for i in range(50000):\n",
        "  action = np.argmax(policy[state])\n",
        "  print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  # print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  state = discretize_observation(obs)\n",
        "  # screen = env.render(mode='rgb_array')\n",
        "\n",
        "  # plt.imshow(screen)\n",
        "  # ipythondisplay.clear_output(wait=True)\n",
        "  # ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    print (\"done\")\n",
        "    break\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "metadata": {
        "id": "mLYhCjxo9S47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "31c6042a-506e-4518-9cb1-1e2745530e1b"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 reward:  0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 1 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 2 reward:  1.0  state:  (2, 4, 4, 3)  action= 1\n",
            "step 3 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 4 reward:  1.0  state:  (2, 4, 4, 3)  action= 1\n",
            "step 5 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 6 reward:  1.0  state:  (2, 4, 4, 3)  action= 1\n",
            "step 7 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 8 reward:  1.0  state:  (2, 4, 3, 3)  action= 1\n",
            "step 9 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 10 reward:  1.0  state:  (2, 4, 3, 3)  action= 1\n",
            "step 11 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 12 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 13 reward:  1.0  state:  (2, 3, 2, 3)  action= 0\n",
            "step 14 reward:  1.0  state:  (2, 3, 2, 3)  action= 0\n",
            "step 15 reward:  1.0  state:  (2, 3, 2, 3)  action= 0\n",
            "step 16 reward:  1.0  state:  (2, 2, 2, 3)  action= 0\n",
            "step 17 reward:  1.0  state:  (2, 2, 2, 3)  action= 0\n",
            "step 18 reward:  1.0  state:  (2, 2, 2, 4)  action= 1\n",
            "step 19 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 20 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 21 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 22 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 23 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 24 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 25 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 26 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 27 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 28 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 29 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 30 reward:  1.0  state:  (2, 3, 4, 2)  action= 0\n",
            "step 31 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 32 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 33 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 34 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 35 reward:  1.0  state:  (2, 1, 3, 3)  action= 1\n",
            "step 36 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 37 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 38 reward:  1.0  state:  (2, 1, 4, 4)  action= 1\n",
            "step 39 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 40 reward:  1.0  state:  (2, 1, 4, 4)  action= 1\n",
            "step 41 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 42 reward:  1.0  state:  (2, 1, 4, 4)  action= 1\n",
            "step 43 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 44 reward:  1.0  state:  (2, 1, 5, 4)  action= 1\n",
            "step 45 reward:  1.0  state:  (2, 1, 5, 3)  action= 1\n",
            "step 46 reward:  1.0  state:  (2, 2, 5, 3)  action= 0\n",
            "step 47 reward:  1.0  state:  (2, 1, 5, 3)  action= 1\n",
            "step 48 reward:  1.0  state:  (2, 2, 5, 3)  action= 0\n",
            "step 49 reward:  1.0  state:  (2, 1, 5, 3)  action= 1\n",
            "step 50 reward:  1.0  state:  (2, 2, 5, 3)  action= 0\n",
            "step 51 reward:  1.0  state:  (2, 1, 5, 4)  action= 1\n",
            "step 52 reward:  1.0  state:  (2, 2, 6, 3)  action= 1\n",
            "step 53 reward:  1.0  state:  (2, 2, 6, 3)  action= 1\n",
            "step 54 reward:  1.0  state:  (2, 2, 6, 3)  action= 1\n",
            "step 55 reward:  1.0  state:  (2, 3, 6, 3)  action= 1\n",
            "step 56 reward:  1.0  state:  (2, 3, 6, 3)  action= 1\n",
            "step 57 reward:  1.0  state:  (2, 3, 6, 3)  action= 1\n",
            "step 58 reward:  1.0  state:  (2, 4, 6, 2)  action= 0\n",
            "step 59 reward:  1.0  state:  (2, 3, 5, 3)  action= 1\n",
            "step 60 reward:  1.0  state:  (2, 4, 5, 2)  action= 0\n",
            "step 61 reward:  1.0  state:  (2, 3, 5, 3)  action= 1\n",
            "step 62 reward:  1.0  state:  (2, 4, 5, 2)  action= 0\n",
            "step 63 reward:  1.0  state:  (2, 3, 5, 3)  action= 1\n",
            "step 64 reward:  1.0  state:  (2, 4, 5, 3)  action= 0\n",
            "step 65 reward:  1.0  state:  (2, 3, 5, 3)  action= 1\n",
            "step 66 reward:  1.0  state:  (2, 4, 4, 3)  action= 1\n",
            "step 67 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 68 reward:  1.0  state:  (2, 4, 4, 3)  action= 1\n",
            "step 69 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 70 reward:  1.0  state:  (2, 4, 4, 3)  action= 1\n",
            "step 71 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 72 reward:  1.0  state:  (2, 4, 3, 3)  action= 1\n",
            "step 73 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 74 reward:  1.0  state:  (2, 4, 3, 3)  action= 1\n",
            "step 75 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 76 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 77 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 78 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 79 reward:  1.0  state:  (2, 3, 2, 3)  action= 0\n",
            "step 80 reward:  1.0  state:  (2, 2, 2, 3)  action= 0\n",
            "step 81 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 82 reward:  1.0  state:  (2, 2, 3, 4)  action= 0\n",
            "step 83 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 84 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 85 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 86 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 87 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 88 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 89 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 90 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 91 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 92 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 93 reward:  1.0  state:  (2, 3, 4, 2)  action= 0\n",
            "step 94 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 95 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 96 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 97 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 98 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 99 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 100 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 101 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 102 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 103 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 104 reward:  1.0  state:  (2, 3, 4, 2)  action= 0\n",
            "step 105 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 106 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 107 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 108 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 109 reward:  1.0  state:  (2, 1, 3, 3)  action= 1\n",
            "step 110 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 111 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 112 reward:  1.0  state:  (2, 1, 4, 4)  action= 1\n",
            "step 113 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 114 reward:  1.0  state:  (2, 1, 4, 4)  action= 1\n",
            "step 115 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 116 reward:  1.0  state:  (2, 1, 4, 4)  action= 1\n",
            "step 117 reward:  1.0  state:  (2, 1, 4, 3)  action= 0\n",
            "step 118 reward:  1.0  state:  (2, 1, 5, 4)  action= 1\n",
            "step 119 reward:  1.0  state:  (2, 1, 5, 3)  action= 1\n",
            "step 120 reward:  1.0  state:  (2, 2, 5, 3)  action= 0\n",
            "step 121 reward:  1.0  state:  (2, 1, 5, 3)  action= 1\n",
            "step 122 reward:  1.0  state:  (2, 2, 5, 3)  action= 0\n",
            "step 123 reward:  1.0  state:  (2, 1, 5, 3)  action= 1\n",
            "step 124 reward:  1.0  state:  (2, 2, 5, 3)  action= 0\n",
            "step 125 reward:  1.0  state:  (2, 1, 5, 4)  action= 1\n",
            "step 126 reward:  1.0  state:  (2, 2, 6, 3)  action= 1\n",
            "step 127 reward:  1.0  state:  (2, 2, 6, 3)  action= 1\n",
            "step 128 reward:  1.0  state:  (2, 2, 6, 3)  action= 1\n",
            "step 129 reward:  1.0  state:  (2, 3, 6, 3)  action= 1\n",
            "step 130 reward:  1.0  state:  (2, 3, 6, 3)  action= 1\n",
            "step 131 reward:  1.0  state:  (2, 3, 6, 3)  action= 1\n",
            "step 132 reward:  1.0  state:  (2, 4, 5, 2)  action= 0\n",
            "step 133 reward:  1.0  state:  (2, 3, 5, 3)  action= 1\n",
            "step 134 reward:  1.0  state:  (2, 4, 5, 2)  action= 0\n",
            "step 135 reward:  1.0  state:  (2, 3, 5, 3)  action= 1\n",
            "step 136 reward:  1.0  state:  (2, 4, 5, 2)  action= 0\n",
            "step 137 reward:  1.0  state:  (2, 3, 5, 3)  action= 1\n",
            "step 138 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 139 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 140 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 141 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 142 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 143 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 144 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 145 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 146 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 147 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 148 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 149 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 150 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 151 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 152 reward:  1.0  state:  (2, 2, 3, 4)  action= 0\n",
            "step 153 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 154 reward:  1.0  state:  (2, 2, 4, 4)  action= 1\n",
            "step 155 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 156 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 157 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 158 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 159 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 160 reward:  1.0  state:  (2, 4, 4, 2)  action= 0\n",
            "step 161 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 162 reward:  1.0  state:  (2, 4, 3, 2)  action= 0\n",
            "step 163 reward:  1.0  state:  (2, 3, 3, 2)  action= 0\n",
            "step 164 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 165 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 166 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 167 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 168 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 169 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 170 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 171 reward:  1.0  state:  (2, 1, 3, 4)  action= 1\n",
            "step 172 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 173 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 174 reward:  1.0  state:  (2, 2, 4, 3)  action= 1\n",
            "step 175 reward:  1.0  state:  (2, 3, 4, 3)  action= 1\n",
            "step 176 reward:  1.0  state:  (2, 3, 4, 2)  action= 0\n",
            "step 177 reward:  1.0  state:  (2, 3, 3, 3)  action= 0\n",
            "step 178 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 179 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 180 reward:  1.0  state:  (2, 2, 3, 3)  action= 0\n",
            "step 181 reward:  1.0  state:  (2, 1, 3, 3)  action= 1\n",
            "step 182 reward:  1.0  state:  (1, 2, 4, 3)  action= 1\n",
            "step 183 reward:  1.0  state:  (1, 2, 4, 3)  action= 1\n",
            "step 184 reward:  1.0  state:  (1, 2, 4, 3)  action= 1\n",
            "step 185 reward:  1.0  state:  (1, 3, 4, 3)  action= 1\n",
            "step 186 reward:  1.0  state:  (1, 3, 3, 2)  action= 0\n",
            "step 187 reward:  1.0  state:  (1, 3, 3, 3)  action= 1\n",
            "step 188 reward:  1.0  state:  (1, 3, 3, 2)  action= 0\n",
            "step 189 reward:  1.0  state:  (1, 3, 3, 3)  action= 1\n",
            "step 190 reward:  1.0  state:  (1, 3, 3, 2)  action= 0\n",
            "step 191 reward:  1.0  state:  (1, 3, 3, 2)  action= 0\n",
            "step 192 reward:  1.0  state:  (1, 2, 2, 3)  action= 0\n",
            "step 193 reward:  1.0  state:  (1, 2, 2, 3)  action= 0\n",
            "step 194 reward:  1.0  state:  (1, 2, 2, 3)  action= 0\n",
            "step 195 reward:  1.0  state:  (1, 1, 2, 3)  action= 1\n",
            "step 196 reward:  1.0  state:  (1, 2, 2, 3)  action= 0\n",
            "step 197 reward:  1.0  state:  (1, 1, 2, 3)  action= 1\n",
            "step 198 reward:  1.0  state:  (1, 2, 2, 3)  action= 0\n",
            "step 199 reward:  1.0  state:  (1, 1, 2, 3)  action= 1\n",
            "done\n",
            "Iterations that were run: 199\n"
          ]
        }
      ]
    }
  ]
}